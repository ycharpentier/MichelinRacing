{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1a7274",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mise-en-place-de-l'environnement\" data-toc-modified-id=\"Mise-en-place-de-l'environnement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Mise en place de l'environnement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-de-l'Environnement\" data-toc-modified-id=\"Test-de-l'Environnement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Test de l'Environnement</a></span></li></ul></li><li><span><a href=\"#Implémentation-de-l'Algorithme-Deep-Deterministic-Policy-Gradient-(DDPG)\" data-toc-modified-id=\"Implémentation-de-l'Algorithme-Deep-Deterministic-Policy-Gradient-(DDPG)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implémentation de l'Algorithme Deep Deterministic Policy Gradient (DDPG)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Création-de-l'Acteur\" data-toc-modified-id=\"Création-de-l'Acteur-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Création de l'Acteur</a></span></li><li><span><a href=\"#Création-du-Critique\" data-toc-modified-id=\"Création-du-Critique-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Création du Critique</a></span></li><li><span><a href=\"#Création-du-Générateur-de-Bruit\" data-toc-modified-id=\"Création-du-Générateur-de-Bruit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Création du Générateur de Bruit</a></span></li><li><span><a href=\"#Gestion-de-l'Experience-Replay\" data-toc-modified-id=\"Gestion-de-l'Experience-Replay-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Gestion de l'Experience Replay</a></span></li><li><span><a href=\"#Mise-à-jour-des-réseaux-cibles\" data-toc-modified-id=\"Mise-à-jour-des-réseaux-cibles-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Mise à jour des réseaux cibles</a></span></li><li><span><a href=\"#Apprentissage\" data-toc-modified-id=\"Apprentissage-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Apprentissage</a></span></li><li><span><a href=\"#Diagnostique\" data-toc-modified-id=\"Diagnostique-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Diagnostique</a></span></li><li><span><a href=\"#Réglage-des-paramètres-d'apprentissage\" data-toc-modified-id=\"Réglage-des-paramètres-d'apprentissage-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Réglage des paramètres d'apprentissage</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e86dbb",
   "metadata": {},
   "source": [
    "Les modèles (physiques ou corrélatifs) des pneumatiques jouent un rôle essentiel dans la mise au points de scénarios de conception ainsi que dans l'évaluation des performances des nouvelles gammes ou de gammes existantes. Ainsi, le modèle de rigidité de dérive vu dans le TP portant sur l'Optimisation Bayesienne, peut être exploité à travers des chaines de simulation pour juger de la qualité de pneumatiques en terme de critère de comportement, d'adhérence, d'endurance ou encore de temps au tour. C'est à cette performance que nous allons nous intéresser ici.\n",
    "\n",
    "Plus précisément, l'exercice consiste à mettre en place un environnement de simulation basé sur de l'apprentissage par renforcement qui a pour objectif de trouver les controles optimaux à appliquer à un véhicule pour que ce dernier puisse parcourir un circuit circulaire avec la vitesse la plus élevée possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a940b",
   "metadata": {},
   "source": [
    "## Mise en place de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e4ac",
   "metadata": {},
   "source": [
    "En l'occurence, les états que l'on va considérer pour notre environnement sont:\n",
    "- $x$: position du véhicule selon la direction $\\vec{X}$\n",
    "- $y$: position du véhicule selon la direction $\\vec{Y}$\n",
    "- $\\dot{x}$: vitesse du véhicule selon la direction $\\vec{X}$\n",
    "- $\\dot{y}$: vitesse du véhicule selon la direction $\\vec{Y}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Les actions qui seront utilisées sont:\n",
    "- $v$: la vitesse\n",
    "- $\\alpha$: l'angle de braquage\n",
    "\n",
    "L'environnement que l'on va exploiter s'appuie sur le package Gym de la société OpenAI (https://gym.openai.com/). Un tel environnement s'appuie sur l'utilisation d'objets héritant de la classe *gym.Env* et comportant les méthodes suivantes:\n",
    "- **__init__**: constructeur définissant les expaces d'actions (*action_space*) et d'observations (*observation_space*)\n",
    "- **reset**: méthode permettant de réinitialiser les états\n",
    "- **step**: fonction qui prend en entrée les valeurs des actions et renvoie les nouveaux états de l'environnement, le reward ainsi qu'un booléen indiquant s'il est nécessaire de réinitialiser les états \n",
    "- **render**: méthode qui affiche l'état de l'environnement et différentes informations le concernant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5d6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from gym_gmmcar.envs.circle_env import CircleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2984f0",
   "metadata": {},
   "source": [
    "Notre objectif étant de rester sur le cirucuit tout en allant le plus vite possible, quel(-s) reward(-s) peut-on envisager? Implémenter l'un d'entre eux en complétant la méthode *get_reward* de la classe *OttEnv* ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5589fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OttEnv(CircleEnv):\n",
    "    \"\"\"\n",
    "    Environnement de simulation pour une voiture de course suivant une trajectoire circulaire aussi vite que possible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_velocity=1.0,\n",
    "            radius=1.0,\n",
    "            dt=0.035,\n",
    "            model_type='BrushTireModel',\n",
    "            robot_type='RCCar',\n",
    "            mu_s=1.37,\n",
    "            mu_k=1.96,\n",
    "            eps=0.05\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            target_velocity=target_velocity,\n",
    "            radius=radius,\n",
    "            dt=dt,\n",
    "            model_type=model_type,\n",
    "            robot_type=robot_type,\n",
    "            mu_s=mu_s,\n",
    "            mu_k=mu_k\n",
    "        )\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        Définition de la fonction de Reward\n",
    "        \"\"\"\n",
    "        r = self.radius\n",
    "        v = self.target_velocity\n",
    "        x, y, _, x_dot, y_dot, _ = state\n",
    "        vitesse = np.sqrt(x_dot**2 + y_dot**2)\n",
    "        distance = np.sqrt(x**2 + y**2)\n",
    "\n",
    "        # Reward à définir\n",
    "        distance_penalty = - ((distance - r) / r)**2  \n",
    "        vitesse_penalty = - ((vitesse - v) / v)**2\n",
    "        reward = 1 + distance_penalty + vitesse_penalty\n",
    "        \n",
    "        info = {}\n",
    "        info['dist'] = distance\n",
    "        info['vel'] = vitesse\n",
    "        return reward, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc0eca3-80b6-4ad5-bdf4-6aaa2b1d3894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0.        -0.5235988], [10.         0.5235988], (2,), float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cb961-db71-4c8b-ab38-43c4b472827b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6ec332a1-31a2-4bee-8da7-2fe244d915be",
   "metadata": {},
   "source": [
    "Ajuster selon l'objectif : $r = 1 \\gamma d $ rester circuit ++  penalty -100 / bande autour du curcuit [-0.02,0.02]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2b78",
   "metadata": {},
   "source": [
    "### Test de l'Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c92cd",
   "metadata": {},
   "source": [
    "Tester l'environnement en considérant un épisode de 100 pas de temps et des actions aléatoires et/ou fixes. Pour ce faire, compléter le script ci-dessous en définissant les actions à appliquer à chaque pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OttEnv()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "episode = 1\n",
    "for step in range(100):\n",
    "    #action = env.action_space.sample()\n",
    "    action = np.array([100,1])\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efff82c",
   "metadata": {},
   "source": [
    "Etant donné les caractéristiques du problème considéré, quel type de méthode devrait-on appliquer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefea573",
   "metadata": {},
   "source": [
    "## Implémentation de l'Algorithme Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbb3c1-b9d8-481c-ace2-dc30f968338d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ad190",
   "metadata": {},
   "source": [
    "Pour tenter de trouver les commandes optimales à appliquer, nous allons ici utiliser une approche DDPG. Pour ce faire, la première étape à réaliser est d'implémenter cette méthode en s'appuyant sur le pseudo-code suivant vu en cours:\n",
    "![DDPG.png](DDPG.png \"Algorithme DDPG\")\n",
    "\n",
    "\n",
    "/!\\ bien faire step by step, revoir le **cours**, acteur critique, DQL actions discrètes => problème car espace d'actions continu; pour résourdre ce problème => acteur critique + DQL ~= DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d06dda",
   "metadata": {},
   "source": [
    "### Création de l'Acteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d82702",
   "metadata": {},
   "source": [
    "Pour rappel, l'acteur a pour objectif d'estimer un politique $\\mu(s_{t})$. Dans un premier temps, créer un acteur à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow ayant l'architecture suivante:\n",
    "- une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "- une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "- une couche de sortie dense comportant un nombre de neurones égal au nombre d'actions et une fonction d'activation de type tanh\n",
    "\n",
    "<ins>**Remarque:**</ins> Les sorties étant bornées entre -1 et 1, ne pas oublier de dénormaliser pour générer des valeurs d'actions conformes à l'espace des actions.\n",
    "<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bc5bece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_actor(num_actions, action_bounds, num_neurons=256, state_dim=4):\n",
    "    inputs = layers.Input(shape=(state_dim,), name=\"state\")\n",
    "    x = layers.Dense(num_neurons, activation='relu')(inputs)\n",
    "    x = layers.Dense(num_neurons, activation='relu')(x)\n",
    "    normalized_actions = layers.Dense(num_actions, activation='tanh')(x)\n",
    "\n",
    "    min_action, max_action = action_bounds # on dénomalise l'output\n",
    "    scaled_actions = layers.Lambda(\n",
    "        lambda x: min_action + (x + 1.0) * (max_action - min_action) / 2.0,\n",
    "        name=\"scaled_actions\"\n",
    "    )(normalized_actions)\n",
    "\n",
    "    model = tf.keras.Model(inputs, scaled_actions, name=\"actor_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec96a609-42ea-44e3-a636-11f6611f363c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"actor_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"actor_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ state (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_275 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_276 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_277 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ scaled_actions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ state (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_275 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_276 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_277 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m514\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ scaled_actions (\u001b[38;5;33mLambda\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,586</span> (264.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,586\u001b[0m (264.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,586</span> (264.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,586\u001b[0m (264.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actor_model = create_actor(num_actions=2, action_bounds=action_bounds)\n",
    "\n",
    "\n",
    "\n",
    "actor_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7508c",
   "metadata": {},
   "source": [
    "### Création du Critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3482374",
   "metadata": {},
   "source": [
    "Pour rappel, le critique a pour objectif d'estimer la valeur $Q(s_{t},a_{t})$Dans un premier temps, créer un critique à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow de la manière suivante:\n",
    "- Créer un réseau prenant en entrée les états avec:\n",
    "  - une première couche cachée dense comportant 16 neurones et une fonction d'activation de type RELU\n",
    "  - une seconde couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n",
    "- Créer un réseau prenant en entrée les actions avec une couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n",
    "- Concaténer les sorties des 2 réseaux précédents via la méthode \"*Concatenate*\"\n",
    "- Créer un réseau prenant les entrées la concaténation des tenseurs précédents avec:\n",
    "  - une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "  - une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "  - une couche de sortie dense comportant 1 neurone sans fonction d'activation \n",
    "\n",
    "<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f56d0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(state_dim, action_dim, num_neurons_state=16, num_neurons_action=32, num_neurons_fc=256):\n",
    "    # etats\n",
    "    state_input = layers.Input(shape=(state_dim,), name=\"state\")\n",
    "    x_state = layers.Dense(num_neurons_state, activation='relu')(state_input)\n",
    "    x_state = layers.Dense(num_neurons_state * 2, activation='relu')(x_state)\n",
    "\n",
    "    # actions\n",
    "    action_input = layers.Input(shape=(action_dim,), name=\"action\")\n",
    "    x_action = layers.Dense(num_neurons_action, activation='relu')(action_input)\n",
    "\n",
    "    # actions  + etats\n",
    "    concatenated = layers.Concatenate()([x_state, x_action])\n",
    "    x = layers.Dense(num_neurons_fc, activation='relu')(concatenated)\n",
    "    x = layers.Dense(num_neurons_fc, activation='relu')(x)\n",
    "    q_value = layers.Dense(1)(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=[state_input, action_input], outputs=q_value, name=\"critic_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ca9c888-a344-40e3-a3bb-b3b066989086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"critic_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"critic_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ state (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_278 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ state[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ action (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_279 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │ dense_278[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_280 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ action[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_279[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ dense_280[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_281 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ concatenate_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_282 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ dense_281[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_283 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dense_282[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ state (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_278 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │              \u001b[38;5;34m80\u001b[0m │ state[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ action (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_279 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │             \u001b[38;5;34m544\u001b[0m │ dense_278[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_280 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │              \u001b[38;5;34m96\u001b[0m │ action[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_33 (\u001b[38;5;33mConcatenate\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense_279[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ dense_280[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_281 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m16,640\u001b[0m │ concatenate_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_282 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │ dense_281[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_283 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │             \u001b[38;5;34m257\u001b[0m │ dense_282[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,409</span> (325.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m83,409\u001b[0m (325.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,409</span> (325.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83,409\u001b[0m (325.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "critic_model = create_critic(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "critic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06becb5e",
   "metadata": {},
   "source": [
    "### Création du Générateur de Bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7ce24",
   "metadata": {},
   "source": [
    "Comme précisé en cours, l'approche DDPG génère les actions de manière déterministe, ce qui engendre mécaniquement une démarche purement basée sur de l'exploitation. Pour éviter d'être coincé dans un optimum local, il est nécessaire d'appliquer une stratégie d'exploration. En l'occurrence, cette exploration est gérée via l'ajout d'un bruit à l'action générée par l'acteur.\n",
    "Ce bruit est généré via un processus stochastique de type ***Ornstein-Uhlenbeck*** défini par l'équation différentielle stochastique:\n",
    "\n",
    "$dx_{t}=\\theta(\\nu-x_{t})dt+\\sigma\\sqrt{d_{t}}u$ avec $u\\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "Créer une fonction ou classe permettant de générer ce bruit avec $\\theta=0.15$ et $d_{t}=1e-2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46121e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du génrateur de bruit\n",
    "def bruit(action_dim, mu=0.0, theta=0.15, sigma=0.2, dt=1e-2):\n",
    "    dx = theta * (mu - state) * dt + sigma * np.sqrt(dt) * np.random.randn(action_dim)\n",
    "    state = state + dx \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd2454",
   "metadata": {},
   "source": [
    "### Gestion de l'Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58d6db",
   "metadata": {},
   "source": [
    "Afin de ne pas oublier les expériences passées et réduire les corrélations entre expériences, un tirage aléatoire de $N$ tuples (état présent, action, reward, état suivant) stockés dans un buffer de taille $B$.\n",
    "Créer une fonction ou classe permettant de:\n",
    "- Initialiser un buffer de taille $B$ à 0\n",
    "- Sauvegarder à chaque pas de temps un 4-uplet (état présent, action, reward, état suivant)\n",
    "- Tirer aléatoirement $N$ tuples (état présent, action, reward, état suivant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a46a7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Buffer_Init(buffer_size, state_dim, action_dim):\n",
    "    buffer = {\n",
    "        'states': np.zeros((buffer_size, state_dim), dtype=np.float32),\n",
    "        'actions': np.zeros((buffer_size, action_dim), dtype=np.float32),\n",
    "        'rewards': np.zeros((buffer_size, 1), dtype=np.float32),\n",
    "        'next_states': np.zeros((buffer_size, state_dim), dtype=np.float32),\n",
    "        'indices': 0}\n",
    "    return buffer\n",
    "\n",
    "def save(buffer, state, action, reward, next_state):\n",
    "    index = buffer['indices']\n",
    "    buffer['states'][index] = state #sauvegarde des états, reward, etatsuivant et action\n",
    "    buffer['actions'][index] = action\n",
    "    buffer['rewards'][index] = reward\n",
    "    buffer['next_states'][index] = next_state\n",
    "    buffer['indices'] = (index + 1) % buffer['states'].shape[0]\n",
    "\n",
    "def random_pick(buffer, batch_size):\n",
    "    indices = np.random.choice(buffer['states'].shape[0], batch_size, replace=False)\n",
    "    states = buffer['states'][indices]\n",
    "    actions = buffer['actions'][indices]\n",
    "    rewards = buffer['rewards'][indices]\n",
    "    next_states = buffer['next_states'][indices]\n",
    "    return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db96a2",
   "metadata": {},
   "source": [
    "### Mise à jour des réseaux cibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4148c",
   "metadata": {},
   "source": [
    "Comme présenté en cours, la gestion des cibles mouvantes se fait via la mise en place de réseaux cibles. En l'occurrence, deux réseaux cibles sont utilisés: l'un pour l'acteur et l'autre pour le critique.\n",
    "Créer une fonction ou classe qui mette à jour les poids des réseaux cibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc74a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à jour des réseaux cibles\n",
    "\n",
    "def update_target_networks(main_network, target_network, tau=0.001):\n",
    "    main_weights = main_network.get_weights() \n",
    "    target_weights = target_network.get_weights()\n",
    "\n",
    "    new_target_weights = [tau * main_weight + (1 - tau) * target_weight for main_weight, target_weight in zip(main_weights, target_weights)] # pas sur sur\n",
    "\n",
    "    target_network.set_weights(new_target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa8330",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8611",
   "metadata": {},
   "source": [
    "Utiliser l'ensembles des fonctions/classes précédemment construites pour implémenter l'apprentissage présenté par le pseudo-code apparaissant plus haut avec les paramètres suivants:\n",
    "- learning rate de l'acteur:0.002\n",
    "- learning rate du critique: 0.001\n",
    "- paramètre du générateur de bruit $\\sigma$: 0.2\n",
    "- paramètre du générateur de bruit $\\nu$: 0\n",
    "- nombre totale d'épisode $M$: 100\n",
    "- facteur d'escompte $\\gamma$: 0.99\n",
    "- paramètre mise à jour des réseaux cible $\\tau$: 0.005\n",
    "- taille du buffer $B$: 1000\n",
    "- taille $N$ des batchs: 100\n",
    "\n",
    "Pour pouvoir mener un diagnosqtique de l'apprentissage, stocker les rewards cumulés à la fin de chaque épisode dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2db2a7ff-0a1e-4101-9697-e61e3c9bf6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0.        -0.5235988], [10.         0.5235988], (2,), float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b5fe23f-1caa-4080-8ed7-9f25da41b8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc6fbb",
   "metadata": {},
   "source": [
    "### Diagnostique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc367ae6",
   "metadata": {},
   "source": [
    "Afficher l'évolution de la moyenne des rewards cumulés calculée tous les 20 épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la moyenne des rewards cumulés\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bad3e6",
   "metadata": {},
   "source": [
    "### Réglage des paramètres d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f40573",
   "metadata": {},
   "source": [
    "Essayer différents paramètres utilisés lors de l'apprentissage ainsi que différentes architecture de réseaux de neurones. Comment pourrait-on automatiser une recherche intelligente de cesdivers paramètres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests et évaluation avec différentes configurations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
