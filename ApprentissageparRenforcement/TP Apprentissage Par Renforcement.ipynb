{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1a7274",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mise-en-place-de-l'environnement\" data-toc-modified-id=\"Mise-en-place-de-l'environnement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Mise en place de l'environnement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-de-l'Environnement\" data-toc-modified-id=\"Test-de-l'Environnement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Test de l'Environnement</a></span></li></ul></li><li><span><a href=\"#Implémentation-de-l'Algorithme-Deep-Deterministic-Policy-Gradient-(DDPG)\" data-toc-modified-id=\"Implémentation-de-l'Algorithme-Deep-Deterministic-Policy-Gradient-(DDPG)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implémentation de l'Algorithme Deep Deterministic Policy Gradient (DDPG)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Création-de-l'Acteur\" data-toc-modified-id=\"Création-de-l'Acteur-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Création de l'Acteur</a></span></li><li><span><a href=\"#Création-du-Critique\" data-toc-modified-id=\"Création-du-Critique-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Création du Critique</a></span></li><li><span><a href=\"#Création-du-Générateur-de-Bruit\" data-toc-modified-id=\"Création-du-Générateur-de-Bruit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Création du Générateur de Bruit</a></span></li><li><span><a href=\"#Gestion-de-l'Experience-Replay\" data-toc-modified-id=\"Gestion-de-l'Experience-Replay-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Gestion de l'Experience Replay</a></span></li><li><span><a href=\"#Mise-à-jour-des-réseaux-cibles\" data-toc-modified-id=\"Mise-à-jour-des-réseaux-cibles-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Mise à jour des réseaux cibles</a></span></li><li><span><a href=\"#Apprentissage\" data-toc-modified-id=\"Apprentissage-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Apprentissage</a></span></li><li><span><a href=\"#Diagnostique\" data-toc-modified-id=\"Diagnostique-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Diagnostique</a></span></li><li><span><a href=\"#Réglage-des-paramètres-d'apprentissage\" data-toc-modified-id=\"Réglage-des-paramètres-d'apprentissage-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Réglage des paramètres d'apprentissage</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e86dbb",
   "metadata": {},
   "source": [
    "Les modèles (physiques ou corrélatifs) des pneumatiques jouent un rôle essentiel dans la mise au points de scénarios de conception ainsi que dans l'évaluation des performances des nouvelles gammes ou de gammes existantes. Ainsi, le modèle de rigidité de dérive vu dans le TP portant sur l'Optimisation Bayesienne, peut être exploité à travers des chaines de simulation pour juger de la qualité de pneumatiques en terme de critère de comportement, d'adhérence, d'endurance ou encore de temps au tour. C'est à cette performance que nous allons nous intéresser ici.\n",
    "\n",
    "Plus précisément, l'exercice consiste à mettre en place un environnement de simulation basé sur de l'apprentissage par renforcement qui a pour objectif de trouver les controles optimaux à appliquer à un véhicule pour que ce dernier puisse parcourir un circuit circulaire avec la vitesse la plus élevée possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a940b",
   "metadata": {},
   "source": [
    "## Mise en place de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e4ac",
   "metadata": {},
   "source": [
    "En l'occurence, les états que l'on va considérer pour notre environnement sont:\n",
    "- $x$: position du véhicule selon la direction $\\vec{X}$\n",
    "- $y$: position du véhicule selon la direction $\\vec{Y}$\n",
    "- $\\dot{x}$: vitesse du véhicule selon la direction $\\vec{X}$\n",
    "- $\\dot{y}$: vitesse du véhicule selon la direction $\\vec{Y}$\n",
    "- $\\psi$: l'angle de lacet du véhicule\n",
    "- $\\dot{\\psi}$: vitesse de lacet du véhicule\n",
    "\n",
    "\n",
    "\n",
    "Les actions qui seront utilisées sont:\n",
    "- $v$: la vitesse\n",
    "- $\\alpha$: l'angle de braquage\n",
    "\n",
    "L'environnement que l'on va exploiter s'appuie sur le package Gym de la société OpenAI (https://gym.openai.com/). Un tel environnement s'appuie sur l'utilisation d'objets héritant de la classe *gym.Env* et comportant les méthodes suivantes:\n",
    "- **__init__**: constructeur définissant les expaces d'actions (*action_space*) et d'observations (*observation_space*)\n",
    "- **reset**: méthode permettant de réinitialiser les états\n",
    "- **step**: fonction qui prend en entrée les valeurs des actions et renvoie les nouveaux états de l'environnement, le reward ainsi qu'un booléen indiquant s'il est nécessaire de réinitialiser les états \n",
    "- **render**: méthode qui affiche l'état de l'environnement et différentes informations le concernant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5d6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from gym_gmmcar.envs.circle_env import CircleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2984f0",
   "metadata": {},
   "source": [
    "Notre objectif étant de rester sur le cirucuit tout en allant le plus vite possible, quel(-s) reward(-s) peut-on envisager? Implémenter l'un d'entre eux en complétant la méthode *get_reward* de la classe *OttEnv* ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5589fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OttEnv(CircleEnv):\n",
    "    \"\"\"\n",
    "    Environnement de simulation pour une voiture de course suivant une trajectoire circulaire aussi vite que possible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_velocity=1.0,\n",
    "            radius=1.0,\n",
    "            dt=0.035,\n",
    "            model_type='BrushTireModel',\n",
    "            robot_type='RCCar',\n",
    "            mu_s=1.37,\n",
    "            mu_k=1.96,\n",
    "            eps=0.05\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            target_velocity=target_velocity,\n",
    "            radius=radius,\n",
    "            dt=dt,\n",
    "            model_type=model_type,\n",
    "            robot_type=robot_type,\n",
    "            mu_s=mu_s,\n",
    "            mu_k=mu_k\n",
    "        )\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        Définition de la fonction de Reward\n",
    "        \"\"\"\n",
    "        r = self.radius\n",
    "        v = self.target_velocity\n",
    "        x, y, _, x_dot, y_dot, _ = state\n",
    "        vitesse = np.sqrt(x_dot**2 + y_dot**2)\n",
    "        distance = np.sqrt(x**2 + y**2)\n",
    "\n",
    "        # Reward à définir\n",
    "        distance_penalty = - ((distance - r) / r)**2  \n",
    "        vitesse_penalty = - ((vitesse - v) / v)**2\n",
    "        reward = 1 + distance_penalty + vitesse_penalty\n",
    "        \n",
    "        info = {}\n",
    "        info['dist'] = distance\n",
    "        info['vel'] = vitesse\n",
    "        return reward, info"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ec332a1-31a2-4bee-8da7-2fe244d915be",
   "metadata": {},
   "source": [
    "Ajuster selon l'objectif : $r = 1 \\gamma d $ rester circuit ++  penalty -100 / bande autour du curcuit [-0.02,0.02]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2b78",
   "metadata": {},
   "source": [
    "### Test de l'Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c92cd",
   "metadata": {},
   "source": [
    "Tester l'environnement en considérant un épisode de 100 pas de temps et des actions aléatoires et/ou fixes. Pour ce faire, compléter le script ci-dessous en définissant les actions à appliquer à chaque pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18d5d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoyoc\\anaconda3\\envs\\michelin\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.29091095 -0.02173952  4.00127525  1.42926365 -0.46067099  2.5398193 ]\n",
      "[-1.33575665 -0.05475184  4.1093056   1.6334305  -0.42530022  3.61851293]\n",
      "[-1.37990306 -0.0990019   4.25358022  1.82524902 -0.47998891  4.61457178]\n",
      "[-1.42183267 -0.15432812  4.43180592  1.98472041 -0.62122258  5.56561953]\n",
      "[-1.45932512 -0.22016624  4.64302938  2.08426745 -0.840745    6.50344375]\n",
      "[-1.48975915 -0.29521034  4.88698598  2.09299329 -1.12425497  7.43532153]\n",
      "[-1.51060519 -0.37720109  5.16327301  1.9811809  -1.4463198   8.34718448]\n",
      "[-1.5199865  -0.46295692  5.47079625  1.72727307 -1.76568389  9.2181665 ]\n",
      "[-1.51713752 -0.54859948  5.80811668  1.32559791 -2.02303392 10.05279357]\n",
      "[-1.50273813 -0.62994153  6.17430884  0.79671376 -2.15020311 10.87183817]\n",
      "[-1.4790865  -0.70312421  6.56924154  0.19489658 -2.0837941  11.69908504]\n",
      "[-1.44968733 -0.76546318  6.9919195  -0.35947731 -1.83827609 12.20697104]\n",
      "[-1.41205094 -0.8175392   7.40155678 -0.66004407 -1.68161973 11.11376237]\n",
      "[-1.36532189 -0.85731276  7.76913183 -0.80678527 -1.4997809   9.87974535]\n",
      "[-1.31380755 -0.88211445  8.09306753 -0.82264279 -1.33736981  8.63788025]\n",
      "[-1.26235891 -0.89116354  8.37483817 -0.74334714 -1.21464124  7.48757887]\n",
      "[-1.21559886 -0.88566375  8.61966945 -0.60761773 -1.12263089  6.54937043]\n",
      "[-1.17727678 -0.86861477  8.83715894 -0.4485956  -1.03656248  5.93990329]\n",
      "[-1.14979595 -0.84408153  9.03935174 -0.28445343 -0.94263796  5.67506137]\n",
      "[-1.13419388 -0.81611562  9.23671804 -0.11678977 -0.85046548  5.64344302]\n",
      "[-1.13038518 -0.78819738  9.43659819  0.03693147 -0.77463318  5.78411384]\n",
      "[-1.13761655 -0.76292033  9.64171231  0.21166919 -0.71994976  5.95071339]\n",
      "[-1.15519708 -0.74302878  9.85418776  0.38220822 -0.68966859  6.20210478]\n",
      "[-1.18131859 -0.73089756 10.0768457   0.54165108 -0.68541092  6.53341816]\n",
      "[-1.21353677 -0.72844499 10.31238139  0.68249593 -0.70849716  6.93726838]\n",
      "[-1.24886762 -0.73693323 10.56322342  0.79612504 -0.75861645  7.40677953]\n",
      "[-1.28393609 -0.7567676  10.83151518  0.87324982 -0.83320695  7.93260791]\n",
      "[-1.31523461 -0.78731839 11.11901628  0.90474928 -0.92648828  8.50196133]\n",
      "[-1.33948879 -0.82683349 11.4269523   0.88333857 -1.02889946  9.09667974]\n",
      "[-1.35409187 -0.87252279 11.75576364  0.80617254 -1.12765582  9.68900514]\n",
      "[-1.35750079 -0.92089158 12.10461524  0.67828643 -1.20967746 10.23131516]\n",
      "[-1.34939014 -0.96834293 12.47037875  0.51604429 -1.26814426 10.63658909]\n",
      "[-1.33031535 -1.01183013 12.8460961   0.34651716 -1.30877619 10.77658866]\n",
      "[-1.30111069 -1.04889518 13.22098612  0.1941712  -1.33883205 10.59087283]\n",
      "[-1.26316599 -1.07700462 13.58502867  0.06859834 -1.351412   10.18326033]\n",
      "[-1.21912619 -1.09387429 13.93291237 -0.03717589 -1.34721423  9.69671083]\n",
      "[-1.17283543 -1.09815877 14.26338224 -0.10053548 -1.3117083   9.18469422]\n",
      "[-1.12859751 -1.08957551 14.57659761 -0.12947397 -1.26056054  8.71494842]\n",
      "[-1.09032908 -1.06949693 14.87411267 -0.12518573 -1.20406429  8.29480875]\n",
      "[-1.0612027  -1.04046938 15.15839675 -0.09471207 -1.14431987  7.96339728]\n",
      "[-1.04335159 -1.00587737 15.43273682 -0.04388199 -1.08508984  7.72944338]\n",
      "[-1.03746120e+00 -9.69349509e-01  1.57008228e+01 -9.74500061e-03\n",
      " -1.04844535e+00  7.60963902e+00]\n",
      "[-1.04167576e+00 -9.33187995e-01  1.59659170e+01 -9.69543062e-03\n",
      " -1.03841966e+00  7.53921344e+00]\n",
      "[-1.05506075e+00 -8.99668252e-01  1.62287194e+01 -9.80837194e-03\n",
      " -1.02983798e+00  7.48099561e+00]\n",
      "[-1.07650511e+00 -8.70963648e-01  1.64895877e+01 -9.45292327e-03\n",
      " -1.02311963e+00  7.42758196e+00]\n",
      "[-1.10443546e+00 -8.48875178e-01  1.67487936e+01 -9.65145721e-03\n",
      " -1.01726932e+00  7.38470220e+00]\n",
      "[-1.13690906e+00 -8.34736273e-01  1.70066028e+01 -9.53208746e-03\n",
      " -1.01219164e+00  7.34763540e+00]\n",
      "[-1.17175093e+00 -8.29344202e-01  1.72632254e+01 -9.94121052e-03\n",
      " -1.00806759e+00  7.31673608e+00]\n",
      "[-1.20668433e+00 -8.32939082e-01  1.75188268e+01 -9.93839753e-03\n",
      " -1.00438422e+00  7.28978771e+00]\n",
      "[-1.23946806e+00 -8.45194051e-01  1.77735509e+01 -9.59143542e-03\n",
      " -1.00090425e+00  7.26702609e+00]\n",
      "[-1.26803114e+00 -8.65243412e-01  1.80275304e+01 -9.79653654e-03\n",
      " -9.98441343e-01  7.24686262e+00]\n",
      "[-1.29058919e+00 -8.91753516e-01  1.82808635e+01 -9.53422476e-03\n",
      " -9.95986579e-01  7.22972051e+00]\n",
      "[-1.30576246e+00 -9.23000413e-01  1.85336497e+01 -9.73560885e-03\n",
      " -9.94098550e-01  7.21533004e+00]\n",
      "[-1.31263229e+00 -9.56983148e-01  1.87859623e+01 -9.76624306e-03\n",
      " -9.92370229e-01  7.20300967e+00]\n",
      "[-1.31079741e+00 -9.91534078e-01  1.90378861e+01 -8.45560698e-03\n",
      " -9.90055930e-01  7.19230879e+00]\n",
      "[-1.30038416e+00 -1.02444285e+00  1.92894860e+01 -6.46545122e-03\n",
      " -9.87762990e-01  7.18419654e+00]\n",
      "[-1.28208412e+00 -1.05361714e+00  1.95408484e+01 -3.96095310e-03\n",
      " -9.85567794e-01  7.17919877e+00]\n",
      "[-1.25709139e+00 -1.07720617e+00  1.97920782e+01 -1.00698589e-03\n",
      " -9.83727393e-01  7.17637207e+00]\n",
      "[-1.22701550e+00 -1.09375389e+00  2.00432668e+01 -3.43768117e-04\n",
      " -9.83481578e-01  7.17790044e+00]\n",
      "[-1.19376287e+00 -1.10230360e+00  2.02945068e+01 -3.76977728e-04\n",
      " -9.83618366e-01  7.17989534e+00]\n",
      "[-1.15942251e+00 -1.10231724e+00  2.05458005e+01 -2.19255603e-04\n",
      " -9.83846154e-01  7.18047831e+00]\n",
      "[-1.12615076e+00 -1.09378988e+00  2.07971355e+01 -2.12776975e-04\n",
      " -9.84101374e-01  7.18112178e+00]\n",
      "[-1.09604186e+00 -1.07725195e+00  2.10485043e+01 -3.76430830e-04\n",
      " -9.84187428e-01  7.18291308e+00]\n",
      "[-1.07098984e+00 -1.05374210e+00  2.12999107e+01 -6.04952277e-05\n",
      " -9.84343651e-01  7.18250207e+00]\n",
      "[-1.05257267e+00 -1.02473541e+00  2.15513428e+01 -1.58422871e-04\n",
      " -9.84355533e-01  7.18400097e+00]\n",
      "[-1.04194933e+00 -9.92055274e-01  2.18028000e+01 -2.36706416e-04\n",
      " -9.84289842e-01  7.18557112e+00]\n",
      "[-1.03979251e+00 -9.57756278e-01  2.20542723e+01 -4.17951372e-05\n",
      " -9.84372785e-01  7.18543346e+00]\n",
      "[-1.04623819e+00 -9.23997010e-01  2.23057645e+01 -1.78686771e-04\n",
      " -9.84405107e-01  7.18650908e+00]\n",
      "[-1.06088350e+00 -8.92903028e-01  2.25572738e+01 -2.74184533e-04\n",
      " -9.85068560e-01  7.18453560e+00]\n",
      "[-1.08280810e+00 -8.66429805e-01  2.28087927e+01 -2.98515495e-04\n",
      " -9.84744851e-01  7.18666336e+00]\n",
      "[-1.11063229e+00 -8.46245441e-01  2.30603276e+01 -1.86491009e-04\n",
      " -9.84741543e-01  7.18678604e+00]\n",
      "[-1.14260671e+00 -8.33620880e-01  2.33118675e+01 -3.12530717e-04\n",
      " -9.84715319e-01  7.18776632e+00]\n",
      "[-1.17671698e+00 -8.29353028e-01  2.35634210e+01 -2.65315716e-04\n",
      " -9.84735719e-01  7.18792988e+00]\n",
      "[-1.21081804e+00 -8.33709669e-01  2.38149798e+01 -1.20598800e-04\n",
      " -9.84940253e-01  7.18684311e+00]\n",
      "[-1.24276143e+00 -8.46417600e-01  2.40665484e+01 -6.52052110e-05\n",
      " -9.84807168e-01  7.18754475e+00]\n",
      "[-1.27053632e+00 -8.66678154e-01  2.43181211e+01 -3.35063702e-04\n",
      " -9.84912927e-01  7.18825690e+00]\n",
      "[-1.29239569e+00 -8.93215730e-01  2.45696957e+01 -2.01303941e-04\n",
      " -9.84833347e-01  7.18834573e+00]\n",
      "[-1.30696045e+00 -9.24358527e-01  2.48212814e+01 -3.54948266e-04\n",
      " -9.84952324e-01  7.18850573e+00]\n",
      "[-1.31331429e+00 -9.58148024e-01  2.50728664e+01 -3.03500785e-04\n",
      " -9.85038344e-01  7.18807397e+00]\n",
      "[-1.31105667e+00 -9.92456559e-01  2.53244513e+01 -1.64631764e-04\n",
      " -9.84899171e-01  7.18832246e+00]\n",
      "[-1.30032849e+00 -1.02512155e+00  2.55760416e+01 -2.29381780e-04\n",
      " -9.84940627e-01  7.18851099e+00]\n",
      "[-1.28167809e+00 -1.05399330e+00  2.58270414e+01  5.96137645e-03\n",
      " -9.81408870e-01  7.18711485e+00]\n",
      "[-1.25646274e+00 -1.07716713e+00  2.60786736e+01  8.59339648e-03\n",
      " -9.80693547e-01  7.19314212e+00]\n",
      "[-1.22629892 -1.09273887 26.33031815  0.06031055 -0.95265487  7.17884074]\n",
      "[-1.19397706 -1.09694478 26.58188477  0.16223109 -0.90859536  7.20880327]\n",
      "[-1.16293345 -1.08936243 26.83588846  0.26506973 -0.8805159   7.30845569]\n",
      "[-1.13642734 -1.07095287 27.0947106   0.36201608 -0.86740938  7.48302093]\n",
      "[-1.11734813 -1.0435737  27.36056027  0.448265   -0.87071649  7.71583475]\n",
      "[-1.10795344 -1.00987279 27.635391    0.51859472 -0.88988367  7.99576243]\n",
      "[-1.10964971 -0.9730711  27.92070275  0.56792721 -0.92273719  8.3128059 ]\n",
      "[-1.12282874 -0.93671626 28.21755922  0.59223714 -0.96602507  8.65281426]\n",
      "[-1.14678254 -0.90434778 28.52645664  0.58916879 -1.0153974   8.99740356]\n",
      "[-1.17974982 -0.87913029 28.84715729  0.55885531 -1.06591438  9.32284749]\n",
      "[-1.2191127  -0.86352278 29.17848905  0.50460196 -1.11297075  9.59953202]\n",
      "[-1.26170977 -0.85906569 29.5181683   0.43295642 -1.15327471  9.79430931]\n",
      "[-1.30416343 -0.86632611 29.86280679  0.3526875  -1.18514814  9.87868425]\n",
      "[-1.34311316 -0.88493306 30.20831724  0.27289735 -1.20792884  9.84033611]\n",
      "[-1.37538103 -0.91359098 30.55062455  0.20030222 -1.21962657  9.70001864]\n",
      "[-1.39818854 -0.95002635 30.8865761   0.14037591 -1.22009884  9.48541939]\n",
      "[-1.40952051 -0.99108536 31.21422217  0.09627108 -1.20972268  9.23209999]\n"
     ]
    }
   ],
   "source": [
    "env = OttEnv()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "episode = 1\n",
    "for step in range(100):\n",
    "    #action = env.action_space.sample()\n",
    "    action = np.array([100,1])\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    #print(new_state)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1041bf-a580-4334-94ad-c89b217538c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f75abf84-af02-4f4a-8c1a-e53887aa47e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state shape: (4,)\n",
      "Next state shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f\"Initial state shape: {state.shape}\")  # etat initial \n",
    "\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"Next state shape: {next_state.shape}\")  # etat suivant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efff82c",
   "metadata": {},
   "source": [
    "Etant donné les caractéristiques du problème considéré, quel type de méthode devrait-on appliquer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefea573",
   "metadata": {},
   "source": [
    "## Implémentation de l'Algorithme Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbb3c1-b9d8-481c-ace2-dc30f968338d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ad190",
   "metadata": {},
   "source": [
    "Pour tenter de trouver les commandes optimales à appliquer, nous allons ici utiliser une approche DDPG. Pour ce faire, la première étape à réaliser est d'implémenter cette méthode en s'appuyant sur le pseudo-code suivant vu en cours:\n",
    "![DDPG.png](DDPG.png \"Algorithme DDPG\")\n",
    "\n",
    "\n",
    "/!\\ bien faire step by step, revoir le **cours**, acteur critique, DQL actions discrètes => problème car espace d'actions continu; pour résourdre ce problème => acteur critique + DQL ~= DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d06dda",
   "metadata": {},
   "source": [
    "### Création de l'Acteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d82702",
   "metadata": {},
   "source": [
    "Pour rappel, l'acteur a pour objectif d'estimer un politique $\\mu(s_{t})$. Dans un premier temps, créer un acteur à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow ayant l'architecture suivante:\n",
    "- une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "- une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "- une couche de sortie dense comportant un nombre de neurones égal au nombre d'actions et une fonction d'activation de type tanh\n",
    "\n",
    "<ins>**Remarque:**</ins> Les sorties étant bornées entre -1 et 1, ne pas oublier de dénormaliser pour générer des valeurs d'actions conformes à l'espace des actions.\n",
    "<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5bece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor(num_actions, action_bounds, num_neurons=256, state_dim=4):\n",
    "    inputs = layers.Input(shape=(state_dim,), name=\"state\")\n",
    "    x = layers.Dense(num_neurons, activation='relu')(inputs)\n",
    "    x = layers.Dense(num_neurons, activation='relu')(x)\n",
    "    normalized_actions = layers.Dense(num_actions, activation='tanh')(x)\n",
    "\n",
    "    min_action, max_action = action_bounds # on dénomalise l'output\n",
    "    scaled_actions = layers.Lambda(\n",
    "        lambda x: min_action + (x + 1.0) * (max_action - min_action) / 2.0,\n",
    "        name=\"scaled_actions\"\n",
    "    )(normalized_actions)\n",
    "\n",
    "    model = tf.keras.Model(inputs, scaled_actions, name=\"actor_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7508c",
   "metadata": {},
   "source": [
    "### Création du Critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3482374",
   "metadata": {},
   "source": [
    "Pour rappel, le critique a pour objectif d'estimer la valeur $Q(s_{t},a_{t})$Dans un premier temps, créer un critique à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow de la manière suivante:\n",
    "- Créer un réseau prenant en entrée les états avec:\n",
    "  - une première couche cachée dense comportant 16 neurones et une fonction d'activation de type RELU\n",
    "  - une seconde couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n",
    "- Créer un réseau prenant en entrée les actions avec une couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n",
    "- Concaténer les sorties des 2 réseaux précédents via la méthode \"*Concatenate*\"\n",
    "- Créer un réseau prenant les entrées la concaténation des tenseurs précédents avec:\n",
    "  - une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "  - une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n",
    "  - une couche de sortie dense comportant 1 neurone sans fonction d'activation \n",
    "\n",
    "<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56d0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(state_dim, action_dim, num_neurons_state=16, num_neurons_action=32, num_neurons_fc=256):\n",
    "    # etats\n",
    "    state_input = layers.Input(shape=(state_dim,), name=\"state\")\n",
    "    x_state = layers.Dense(num_neurons_state, activation='relu')(state_input)\n",
    "    x_state = layers.Dense(num_neurons_state * 2, activation='relu')(x_state)\n",
    "\n",
    "    # actions\n",
    "    action_input = layers.Input(shape=(action_dim,), name=\"action\")\n",
    "    x_action = layers.Dense(num_neurons_action, activation='relu')(action_input)\n",
    "\n",
    "    # actions  + etats\n",
    "    concatenated = layers.Concatenate()([x_state, x_action])\n",
    "    x = layers.Dense(num_neurons_fc, activation='relu')(concatenated)\n",
    "    x = layers.Dense(num_neurons_fc, activation='relu')(x)\n",
    "    q_value = layers.Dense(1)(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=[state_input, action_input], outputs=q_value, name=\"critic_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06becb5e",
   "metadata": {},
   "source": [
    "### Création du Générateur de Bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7ce24",
   "metadata": {},
   "source": [
    "Comme précisé en cours, l'approche DDPG génère les actions de manière déterministe, ce qui engendre mécaniquement une démarche purement basée sur de l'exploitation. Pour éviter d'être coincé dans un optimum local, il est nécessaire d'appliquer une stratégie d'exploration. En l'occurrence, cette exploration est gérée via l'ajout d'un bruit à l'action générée par l'acteur.\n",
    "Ce bruit est généré via un processus stochastique de type ***Ornstein-Uhlenbeck*** défini par l'équation différentielle stochastique:\n",
    "\n",
    "$dx_{t}=\\theta(\\nu-x_{t})dt+\\sigma\\sqrt{d_{t}}u$ avec $u\\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "Créer une fonction ou classe permettant de générer ce bruit avec $\\theta=0.15$ et $d_{t}=1e-2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46121e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noise generator with proper initialization of the state variable\n",
    "def bruit(action_dim, mu=0.0, theta=0.15, sigma=0.2, dt=1e-2):\n",
    "    state = np.zeros(action_dim)  # Initialize the state variable\n",
    "    while True:\n",
    "        dx = theta * (mu - state) * dt + sigma * np.sqrt(dt) * np.random.randn(action_dim)\n",
    "        state = state + dx\n",
    "        yield state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd2454",
   "metadata": {},
   "source": [
    "### Gestion de l'Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58d6db",
   "metadata": {},
   "source": [
    "Afin de ne pas oublier les expériences passées et réduire les corrélations entre expériences, un tirage aléatoire de $N$ tuples (état présent, action, reward, état suivant) stockés dans un buffer de taille $B$.\n",
    "Créer une fonction ou classe permettant de:\n",
    "- Initialiser un buffer de taille $B$ à 0\n",
    "- Sauvegarder à chaque pas de temps un 4-uplet (état présent, action, reward, état suivant)\n",
    "- Tirer aléatoirement $N$ tuples (état présent, action, reward, état suivant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a46a7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Buffer_Init(buffer_size, state_dim, action_dim):\n",
    "    buffer = {\n",
    "        'states': np.zeros((buffer_size, state_dim), dtype=np.float32),\n",
    "        'actions': np.zeros((buffer_size, action_dim), dtype=np.float32),\n",
    "        'rewards': np.zeros((buffer_size, 1), dtype=np.float32),\n",
    "        'next_states': np.zeros((buffer_size, state_dim), dtype=np.float32),\n",
    "        'indices': 0}\n",
    "    return buffer\n",
    "\n",
    "def save(buffer, state, action, reward, next_state):\n",
    "    index = buffer['indices']\n",
    "    buffer['states'][index] = state\n",
    "    buffer['actions'][index] = action\n",
    "    buffer['rewards'][index] = reward\n",
    "    buffer['next_states'][index] = next_state\n",
    "    buffer['indices'] = (index + 1) % buffer['states'].shape[0]\n",
    "\n",
    "def random_pick(buffer, batch_size):\n",
    "    indices = np.random.choice(buffer['states'].shape[0], batch_size, replace=False)\n",
    "    states = buffer['states'][indices]\n",
    "    actions = buffer['actions'][indices]\n",
    "    rewards = buffer['rewards'][indices]\n",
    "    next_states = buffer['next_states'][indices]\n",
    "    return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db96a2",
   "metadata": {},
   "source": [
    "### Mise à jour des réseaux cibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4148c",
   "metadata": {},
   "source": [
    "Comme présenté en cours, la gestion des cibles mouvantes se fait via la mise en place de réseaux cibles. En l'occurrence, deux réseaux cibles sont utilisés: l'un pour l'acteur et l'autre pour le critique.\n",
    "Créer une fonction ou classe qui mette à jour les poids des réseaux cibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc74a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à jour des réseaux cibles\n",
    "\n",
    "def update_target_networks(main_network, target_network, tau):\n",
    "    main_weights = main_network.get_weights() \n",
    "    target_weights = target_network.get_weights()\n",
    "\n",
    "    new_target_weights = [tau * main_weight + (1 - tau) * target_weight for main_weight, target_weight in zip(main_weights, target_weights)] # pas sur sur\n",
    "\n",
    "    target_network.set_weights(new_target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa8330",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8611",
   "metadata": {},
   "source": [
    "Utiliser l'ensembles des fonctions précédemment construites pour implémenter l'apprentissage présenté par le pseudo-code apparaissant plus haut avec les paramètres suivants:\n",
    "- learning rate de l'acteur:0.002\n",
    "- learning rate du critique: 0.001\n",
    "- paramètre du générateur de bruit $\\sigma$: 0.2\n",
    "- paramètre du générateur de bruit $\\nu$: 0\n",
    "- nombre totale d'épisode $M$: 100\n",
    "- facteur d'escompte $\\gamma$: 0.99\n",
    "- paramètre mise à jour des réseaux cible $\\tau$: 0.005\n",
    "- taille du buffer $B$: 1000\n",
    "- taille $N$ des batchs: 100\n",
    "\n",
    "Pour pouvoir mener un diagnosqtique de l'apprentissage, stocker les rewards cumulés à la fin de chaque épisode dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "268a80e9-7251-45e5-9b70-0235bb90ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "ACTOR_LR = 0.001\n",
    "CRITIC_LR = 0.001\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 100\n",
    "EPISODES = 100\n",
    "MAX_STEPS = 200\n",
    "\n",
    "\n",
    "def train_step(buffer, actor, critic, target_actor, target_critic, actor_optimizer, critic_optimizer):\n",
    "    states, actions, rewards, next_states = random_pick(buffer, BATCH_SIZE)\n",
    "\n",
    "    next_actions = target_actor(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "    next_q_values = target_critic([tf.convert_to_tensor(next_states, dtype=tf.float32), next_actions])\n",
    "    target_q_values = rewards + GAMMA * next_q_values * (1.0 - np.array([done for done in rewards]))\n",
    "\n",
    "    #maj du critique \n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = critic([tf.convert_to_tensor(states, dtype=tf.float32), tf.convert_to_tensor(actions, dtype=tf.float32)])\n",
    "        critic_loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
    "    critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "\n",
    "    # maj de l'acteur \n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor(tf.convert_to_tensor(states, dtype=tf.float32))\n",
    "        actor_loss = -tf.reduce_mean(critic([tf.convert_to_tensor(states, dtype=tf.float32), actions]))\n",
    "    actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_ddpg(env):\n",
    "    #init des dimensions et intervalles\n",
    "    state_dim = 6 #env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bounds = (env.action_space.low, env.action_space.high)\n",
    "    #print(state_dim,action_dim,action_bounds)\n",
    "\n",
    "    #init  reseaux\n",
    "    actor = create_actor(action_dim, action_bounds, state_dim=state_dim)\n",
    "    critic = create_critic(state_dim, action_dim)\n",
    "    target_actor = create_actor(action_dim, action_bounds, state_dim=state_dim)\n",
    "    target_critic = create_critic(state_dim, action_dim)\n",
    "\n",
    "    target_actor.set_weights(actor.get_weights())\n",
    "    target_critic.set_weights(critic.get_weights())\n",
    "\n",
    "    # optim\n",
    "    actor_optimizer = tf.keras.optimizers.Adam(ACTOR_LR)\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(CRITIC_LR)\n",
    "\n",
    "    #init buffer\n",
    "    buffer = Buffer_Init(BUFFER_SIZE, state_dim, action_dim)\n",
    "\n",
    "    #bruit pour exploration \n",
    "    noise_gen = bruit(action_dim)\n",
    "\n",
    "    # Boucle d'apprentissage\n",
    "    for episode in range(EPISODES):\n",
    "        env.reset()\n",
    "        state = env._state\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            state_tensor = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), axis=0)\n",
    "            action = actor(state_tensor).numpy()[0] + next(noise_gen)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(next_state)\n",
    "            save(buffer, state, action, reward, next_state)\n",
    "\n",
    "            if buffer['indices'] >= BATCH_SIZE:\n",
    "                train_step(buffer, actor, critic, target_actor, target_critic, actor_optimizer, critic_optimizer)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # maj des paramètres\n",
    "        update_target_networks(actor, target_actor, TAU)\n",
    "        update_target_networks(critic, target_critic, TAU)\n",
    "\n",
    "        print(f\"Épisode {episode + 1}, Reward total = {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "55bd87d3-3bfe-480b-b076-9b1377e9e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1, Reward total = -3615.7797544156438\n",
      "Épisode 2, Reward total = -4.864676471436246\n",
      "Épisode 3, Reward total = 9.093496261752504\n",
      "Épisode 4, Reward total = 35.780727284235056\n",
      "Épisode 5, Reward total = 17.06023856648893\n",
      "Épisode 6, Reward total = 21.535845948119842\n",
      "Épisode 7, Reward total = 37.33985626136411\n",
      "Épisode 8, Reward total = 25.02907288401775\n",
      "Épisode 9, Reward total = 8.060994184200867\n",
      "Épisode 10, Reward total = 16.610842398396535\n",
      "Épisode 11, Reward total = 19.93170731563263\n",
      "Épisode 12, Reward total = 31.954791027001228\n",
      "Épisode 13, Reward total = 23.75888623958979\n",
      "Épisode 14, Reward total = 2.6533494010540797\n",
      "Épisode 15, Reward total = 4.217466843908708\n",
      "Épisode 16, Reward total = 9.395423095253555\n",
      "Épisode 17, Reward total = 33.56006114006121\n",
      "Épisode 18, Reward total = -9.589848938906664\n",
      "Épisode 19, Reward total = 12.722309712390096\n",
      "Épisode 20, Reward total = 25.8498951155992\n",
      "Épisode 21, Reward total = 6.892770732760539\n",
      "Épisode 22, Reward total = 22.211933443407982\n",
      "Épisode 23, Reward total = 1.8432301474581014\n",
      "Épisode 24, Reward total = -0.4474841254693358\n",
      "Épisode 25, Reward total = 3.0151043927012116\n",
      "Épisode 26, Reward total = -2.1329223963259496\n",
      "Épisode 27, Reward total = -4.958330502248269\n",
      "Épisode 28, Reward total = 1.326993921151256\n",
      "Épisode 29, Reward total = 2.1846769489752416\n",
      "Épisode 30, Reward total = -0.4336785584067634\n",
      "Épisode 31, Reward total = 3.5275243246410923\n",
      "Épisode 32, Reward total = 3.317253535770324\n",
      "Épisode 33, Reward total = 2.917949311054652\n",
      "Épisode 34, Reward total = -2.448932392059084\n",
      "Épisode 35, Reward total = 2.6816406622156976\n",
      "Épisode 36, Reward total = 2.458743651316893\n",
      "Épisode 37, Reward total = 4.766015771994736\n",
      "Épisode 38, Reward total = -2.698700920061858\n",
      "Épisode 39, Reward total = 0.8826503909500998\n",
      "Épisode 40, Reward total = 6.532954869821471\n",
      "Épisode 41, Reward total = -2.529652762602088\n",
      "Épisode 42, Reward total = 0.021524378107048325\n",
      "Épisode 43, Reward total = 4.7270063273591\n",
      "Épisode 44, Reward total = 10.64970571905145\n",
      "Épisode 45, Reward total = 15.491599249409372\n",
      "Épisode 46, Reward total = 24.67094006207252\n",
      "Épisode 47, Reward total = -16.966025706024745\n",
      "Épisode 48, Reward total = 8.187144062547853\n",
      "Épisode 49, Reward total = 0.39171523592365387\n",
      "Épisode 50, Reward total = 9.691612168449813\n",
      "Épisode 51, Reward total = 29.848578945862744\n",
      "Épisode 52, Reward total = 15.376500005373607\n",
      "Épisode 53, Reward total = 9.982601459110125\n",
      "Épisode 54, Reward total = 3.394314418071931\n",
      "Épisode 55, Reward total = 5.734867939732875\n",
      "Épisode 56, Reward total = 2.848456777872358\n",
      "Épisode 57, Reward total = -2.132184096926163\n",
      "Épisode 58, Reward total = 6.2312298153629335\n",
      "Épisode 59, Reward total = -2.151378744394253\n",
      "Épisode 60, Reward total = 5.128480126249448\n",
      "Épisode 61, Reward total = 19.093643871249032\n",
      "Épisode 62, Reward total = 4.021332319770966\n",
      "Épisode 63, Reward total = 0.29538184501952935\n",
      "Épisode 64, Reward total = 2.649826053598471\n",
      "Épisode 65, Reward total = 1.5319289554214663\n",
      "Épisode 66, Reward total = 4.752329428623776\n",
      "Épisode 67, Reward total = 3.1795724672237893\n",
      "Épisode 68, Reward total = 6.202111898639244\n",
      "Épisode 69, Reward total = -4.8194146987264395\n",
      "Épisode 70, Reward total = 3.8134746167157956\n",
      "Épisode 71, Reward total = 2.7528166933256855\n",
      "Épisode 72, Reward total = -1.82177447225235\n",
      "Épisode 73, Reward total = -11.275500654952534\n",
      "Épisode 74, Reward total = 2.9714970153026714\n",
      "Épisode 75, Reward total = 2.3027694295829715\n",
      "Épisode 76, Reward total = -1.2131664440134196\n",
      "Épisode 77, Reward total = -1.0333978929005845\n",
      "Épisode 78, Reward total = -0.8800178301484547\n",
      "Épisode 79, Reward total = 1.0198831881010442\n",
      "Épisode 80, Reward total = -1.789374569757212\n",
      "Épisode 81, Reward total = 2.628000273835674\n",
      "Épisode 82, Reward total = 2.4258793235399363\n",
      "Épisode 83, Reward total = 0.4892234792994926\n",
      "Épisode 84, Reward total = 1.899062619656798\n",
      "Épisode 85, Reward total = -6.4146901964371015\n",
      "Épisode 86, Reward total = 17.847984687250694\n",
      "Épisode 87, Reward total = 25.97472499942336\n",
      "Épisode 88, Reward total = 7.94808215405541\n",
      "Épisode 89, Reward total = 12.19339857042852\n",
      "Épisode 90, Reward total = 38.672460794746165\n",
      "Épisode 91, Reward total = 39.755028995143775\n",
      "Épisode 92, Reward total = 29.397140319886258\n",
      "Épisode 93, Reward total = 43.70444969568585\n",
      "Épisode 94, Reward total = -22.103429199938404\n",
      "Épisode 95, Reward total = 51.5020373353625\n",
      "Épisode 96, Reward total = 16.905871528790502\n",
      "Épisode 97, Reward total = -1.1914301401438914\n",
      "Épisode 98, Reward total = 42.08773162178722\n",
      "Épisode 99, Reward total = 40.6777508324379\n",
      "Épisode 100, Reward total = 47.50699395877358\n"
     ]
    }
   ],
   "source": [
    "env = OttEnv()\n",
    "train_ddpg(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92c0da48-c286-42fc-bed5-6f6d1d1c2ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21152265, -0.06062286,  4.9144126 ,  1.90897533, -0.14733457,\n",
       "       -0.55039337])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env._state\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc6fbb",
   "metadata": {},
   "source": [
    "### Diagnostique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc367ae6",
   "metadata": {},
   "source": [
    "Afficher l'évolution de la moyenne des rewards cumulés calculée tous les 20 épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2170fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la moyenne des rewards cumulés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bad3e6",
   "metadata": {},
   "source": [
    "### Réglage des paramètres d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f40573",
   "metadata": {},
   "source": [
    "Essayer différents paramètres utilisés lors de l'apprentissage ainsi que différentes architecture de réseaux de neurones. Comment pourrait-on automatiser une recherche intelligente de cesdivers paramètres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8102ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests et évaluation avec différentes configurations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
